# (c) 2017 DataNexus Inc.  All Rights Reserved
---
# First, setup a couple of facts so that can test whether or not a local Confluent
# repository link was provided and we also know if we're installing Confluent
# from a local directory on the Ansible node (or not)
- set_fact:
    install_from_dir: "{{not(local_kafka_file is undefined or local_kafka_file is none or local_kafka_file | trim == '')}}"
- set_fact:
    install_from_url: "{{not(kafka_url is undefined or kafka_url is none or kafka_url | trim == '')}}"
  when: not(install_from_dir)
# if we're not installing confluent from a local directory or an RPM bundle
# (passed in via the 'kafka_url' parameter), then we should prepare to install
# Kafka from the standrad Confluent repository, then installing the packages
# needed from that repository
- block:
  - name: "Add the confluent RPM key to our list of trusted RPM keys"
    rpm_key:
      key: "https://packages.confluent.io/rpm/{{confluent_version}}/archive.key"
      state: present
      validate_certs: no
    environment: "{{environment_vars}}"
  - name: Create a confluent yum repository file
    template:
      src: ../templates/confluent-repo.j2
      dest: /etc/yum.repos.d/confluent.repo
      mode: 0644
  - name: Install confluent from repository
    yum:
      name: "{{confluent_package_name}}"
      state: present
    environment: "{{environment_vars}}"
  become: true
  when: not(install_from_dir) and (install_from_url is undefined or not(install_from_url))
# otherwise, if we're installing from a bundled set of URLs that will be
# downloaded from a URL or we're installing from a set of RPM files in a local
# directory on the Ansible node that we're running this playbook from; in the
# first case we need to download and unpack the bundled RPM files into a '/tmp'
# directory, while in the second we need to copy over the files from that
# directory to a '/tmp' directory; in both cases we then need to install the
# Confluent packages from that directory locally
- block:
  - name: Download the RPM bundle to the /tmp directory
    get_url:
      url: "{{kafka_url}}"
      dest: /tmp
      mode: 0644
      validate_certs: no
    environment: "{{environment_vars}}"
  - set_fact:
      local_filename: "{{kafka_url | basename}}"
  become: true
  when: install_from_url is defined and install_from_url
- block:
  - name: Copy confluent files from a local directory to /tmp
    copy:
      src: "{{local_kafka_file}}"
      dest: "/tmp"
      mode: 0644
  - set_fact:
      local_filename: "{{local_kafka_file | basename}}"
  become: true
  when: install_from_dir
- block:
  - name: Unpack kafka distribution into directory in /tmp
    unarchive:
      copy: no
      src: "/tmp/{{local_filename}}"
      dest: "/tmp"
      list_files: yes
    register: unarchive_out
  - set_fact:
      archive_dirname: "{{unarchive_out.files[0]}}"
  - name: Get a list of the packages copied over
    find:
      paths: "/tmp/{{archive_dirname}}"
      patterns: "*.rpm"
    register: rpm_list
  - name: Install all of the packages copied over
    yum:
      name: "{{rpm_list.files | map(attribute='path') | list | join(',')}}"
      state: present
  become: true
  when: install_from_dir or (install_from_url is defined and install_from_url)
# Now that we've installed the packages that we need, setup the appropriate `log`
# directories for Kafka (and for Zookeeper in the case of a single-node deployment);
# finally, finish by setting up some facts that we'll need later in our playbook
- name: Set fact for the log directory location
  set_fact: log_dir_location="{{kafka_data_dir | default('/var/lib')}}"
- block:
  - name: "Create {{log_dir_location}}/kafka directory"
    become: true
    file:
      path: "{{log_dir_location}}/kafka"
      state: directory
      owner: kafka
      group: kafka
  - name: Create /var/log/kafka directory
    file:
      path: /var/log/kafka
      state: directory
      owner: kafka
      group: kafka
  - name: "Create {{log_dir_location}}/zookeeper directory"
    file:
      path: "{{log_dir_location}}/zookeeper"
      state: directory
      owner: kafka
      group: kafka
    when: (zk_nodes | default([])) == []
  become: true
# cleanup the temporary files created
- name: Remove downloaded RPM bundle
  file:
    path: "/tmp/{{archive_dirname}}"
    state: absent
  become: true
  when: install_from_url is defined and install_from_url
- name: Remove directory created in /tmp directory
  file:
    path: "/tmp/{{local_filename}}"
    state: absent
  become: true
  when: install_from_dir or (install_from_url is defined and install_from_url)
# finally, set values for kafka_bin_dir, kafka_config_dir, kafka_topics_cmd, and
# schema_registry_config_dir
- name: Set a few facts that are used later in the playbook
  set_fact:
    kafka_bin_dir: "/usr/bin"
    kafka_config_dir: "/etc/kafka"
    kafka_topics_cmd: "kafka-topics"
    schema_registry_config_dir: "/etc/schema-registry"
